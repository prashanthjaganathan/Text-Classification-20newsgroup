{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification Using ML/DL on the 20 Newsgroups Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will be working with a publicly available dataset consisting of documents from 20 different Usenet newsgroups. The task is to design, implement, and train two types of classifiers — a traditional machine learning model (Naive Bayes and Logistic Regression) and a deep learning model (Convolutional Neural Network, or CNN) — to classify these documents into their corresponding newsgroups.\n",
    "\n",
    "The dataset can be found [here](http://qwone.com/~jason/20Newsgroups/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About the dataset\n",
    "\n",
    "The 20 newsgroup dataset sorted by date version contains 18846 documents. However, the matlab version which contains the `train.data`, `test.data`, etc. files contain 18774 documents as it was processessed on a script that removes single-word and empty documents from rainbow to matlab.\n",
    "\n",
    "Here, we will pre-process the data from scratch and create the `train.data`, `test.data`, etc.\n",
    "\n",
    "First, let's download the `20news-bydate` dataset using link provided above, and load it in the below format:\n",
    "```json\n",
    "{\n",
    "    \"data\": [\"doc1\", \"doc2\", .... ],\n",
    "    \"target\": [\"label_id1\", \"label_id2\", ..... ],\n",
    "    \"target_names\": [\"label_name1\", \"label_name2\", ..... ],\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This was obtained from the `.map` files\n",
    "\n",
    "classes_to_idx = {\n",
    "    'alt.atheism': 0,\n",
    "    'comp.graphics': 1,\n",
    "    'comp.os.ms-windows.misc': 2,\n",
    "    'comp.sys.ibm.pc.hardware': 3,\n",
    "    'comp.sys.mac.hardware': 4,\n",
    "    'comp.windows.x': 5,\n",
    "    'misc.forsale': 6,\n",
    "    'rec.autos': 7,\n",
    "    'rec.motorcycles': 8,\n",
    "    'rec.sport.baseball': 9,\n",
    "    'rec.sport.hockey': 10,\n",
    "    'sci.crypt': 11,\n",
    "    'sci.electronics': 12,\n",
    "    'sci.med': 13,\n",
    "    'sci.space': 14,\n",
    "    'soc.religion.christian': 15,\n",
    "    'talk.politics.guns': 16,\n",
    "    'talk.politics.mideast': 17,\n",
    "    'talk.politics.misc': 18,\n",
    "    'talk.religion.misc': 19\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def create_dataset_dict(base_path):\n",
    "    \"\"\"\n",
    "    Create a dictionary for a given folder (either 'train' or 'test')\n",
    "    that contains 'data', 'target_names', and 'target'.\n",
    "    \"\"\"\n",
    "    dataset = {\n",
    "        'data': [],  # Will contain document contents\n",
    "        'target_names': [],  # Will contain class labels\n",
    "        'target': []  # Will contain corresponding label ids\n",
    "    }\n",
    "\n",
    "    # Traverse each folder (representing a class)\n",
    "    for label_folder in os.listdir(base_path):\n",
    "        label_folder_path = os.path.join(base_path, label_folder)\n",
    "        if os.path.isdir(label_folder_path):\n",
    "            # Add the label to the target_names\n",
    "            dataset['target_names'].append(label_folder)\n",
    "            \n",
    "            # Get the label id from the label_map\n",
    "            label_id = classes_to_idx[label_folder]\n",
    "            \n",
    "            # Add the documents in this folder to the dataset\n",
    "            for doc_file in os.listdir(label_folder_path):\n",
    "                doc_file_path = os.path.join(label_folder_path, doc_file)\n",
    "                if os.path.isfile(doc_file_path):\n",
    "                    with open(doc_file_path, 'r', encoding='latin1') as f:\n",
    "                        document = f.read()\n",
    "                    dataset['data'].append(document)\n",
    "                    dataset['target'].append(label_id)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "# Set your paths to the train and test folders\n",
    "train_folder = '20news-bydate/20news-bydate-train'\n",
    "test_folder = '20news-bydate/20news-bydate-test'\n",
    "\n",
    "# Create the trainset and testset\n",
    "trainset = create_dataset_dict(train_folder)\n",
    "testset = create_dataset_dict(test_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First document in trainset: From: news@cbnewsk.att.com\n",
      "Subject: Re: Bible Unsuitable for New Christians\n",
      "Organization: AT&T Bell Labs\n",
      "Lines: 8\n",
      "\n",
      "True.\n",
      "\n",
      "Also read 2 Peter 3:16\n",
      "\n",
      "Peter warns that the scriptures are often hard to understand by those who\n",
      "are not learned on the subject.\n",
      "\n",
      "Joe Moore\n",
      "\n",
      "First label in trainset: 17\n",
      "First label name in trainset: talk.politics.mideast\n"
     ]
    }
   ],
   "source": [
    "print(f\"First document in trainset: {trainset['data'][10000]}\")\n",
    "print(f\"First label in trainset: {trainset['target'][0]}\")\n",
    "print(f\"First label name in trainset: {trainset['target_names'][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18846"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainset['data']) + len(testset['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: jake@bony1.bony.com (Jake Livni)\n",
      "Subject: Re: Basil, opinions? (Re: Water on the brain)\n",
      "Organization: The Department of Redundancy Department\n",
      "Lines: 15\n",
      "\n",
      "In article <1qmr5qINN5af@early-bird.think.com> shaig@Think.COM (Shai Guday) writes:\n",
      "\n",
      ">The Litani river flows in a west-southwestern direction and indeed does\n",
      ">not run through the buffer zone.  The Hasbani does flow into the Jordan\n",
      ">but contrary to what our imaginative poster might write, there has been\n",
      ">no increase in the inflow from this river that is not proportional to\n",
      ">climatic changes in rainfall.\n",
      "\n",
      "What did you have to go and bring THAT up for?  Now they're going to\n",
      "say that Israel is stealing the RAIN, too....\n",
      "\n",
      "-- \n",
      "Jake Livni  jake@bony1.bony.com           Ten years from now, George Bush will\n",
      "American-Occupied New York                   have replaced Jimmy Carter as the\n",
      "My opinions only - employer has no opinions.    standard of a failed President.\n",
      "\n",
      "17\n",
      "LABEL: soc.religion.christian ----END----\n",
      "\n",
      "\n",
      "From: alan@apple.com (Alan Mimms)\n",
      "Subject: Re: UART needed (really BREAK detect)\n",
      "Organization: Apple Computer, Inc.\n",
      "Lines: 26\n",
      "\n",
      "In article <jam.19.735404158@ameslab.gov>, jam@ameslab.gov (Jerry\n",
      "Musselman) wrote:\n",
      "> \n",
      "> I need to find a UART that will interface to an 8051 and do the following:\n",
      "> \t-250k baud, 8 data bits, 2 stop bits, no parity\n",
      "> \t-ability to do BREAK detect (IRQ or output pin)\n",
      "> \t-IRQ on character received\n",
      "> \n",
      "> I'm using a Dallas DS2250 at 16 Mhz (8051 clone), but it won't do \n",
      "> break detect.  I've looked at the 6850, 8251, 7201, 2661, etc...\n",
      "> \n",
      "> \tAny help would be appriciated!!!\n",
      "\n",
      "Actually detecting a BREAK is done by watching for a \"character\" containing\n",
      "all zero bits with the framing error resulting from its receipt.  This\n",
      "means that the line stayed in the zero bit state even past the stop bit\n",
      "time slot, which basically indicates a BREAK.  There is no special way to\n",
      "detect BREAK that I have found other than this -- there's no magic signal\n",
      "generated by UARTs, etc.\n",
      "\n",
      "Alan Mimms (alan@apple.com, ...!apple!alan)   | My opinions are generally\n",
      "Portable Macintosh Software Group             | pretty worthless, but\n",
      "Apple Computer                                | they *are* my own...\n",
      "Art without engineering is dreaming.  Engineering without art is\n",
      "calculating.\n",
      "\t-- Steven K. Roberts in \"Computing Across America\"\n",
      "\n",
      "12\n",
      "LABEL: comp.graphics ----END----\n",
      "\n",
      "\n",
      "From: I3150101@dbstu1.rz.tu-bs.de (Benedikt Rosenau)\n",
      "Subject: Re: Gospel Dating\n",
      "Organization: Technical University Braunschweig, Germany\n",
      "Lines: 93\n",
      "\n",
      "In article <65974@mimsy.umd.edu>\n",
      "mangoe@cs.umd.edu (Charley Wingate) writes:\n",
      " \n",
      ">>Well, John has a quite different, not necessarily more elaborated theology.\n",
      ">>There is some evidence that he must have known Luke, and that the content\n",
      ">>of Q was known to him, but not in a 'canonized' form.\n",
      ">\n",
      ">This is a new argument to me.  Could you elaborate a little?\n",
      ">\n",
      " \n",
      "The argument goes as follows: Q-oid quotes appear in John, but not in\n",
      "the almost codified way they were in Matthew or Luke. However, they are\n",
      "considered to be similar enough to point to knowledge of Q as such, and\n",
      "not an entirely different source.\n",
      " \n",
      " \n",
      ">>Assuming that he knew Luke would obviously put him after Luke, and would\n",
      ">>give evidence for the latter assumption.\n",
      ">\n",
      ">I don't think this follows.  If you take the most traditional attributions,\n",
      ">then Luke might have known John, but John is an elder figure in either case.\n",
      ">We're talking spans of time here which are well within the range of\n",
      ">lifetimes.\n",
      " \n",
      "We are talking date of texts here, not the age of the authors. The usual\n",
      "explanation for the time order of Mark, Matthew and Luke does not consider\n",
      "their respective ages. It says Matthew has read the text of Mark, and Luke\n",
      "that of Matthew (and probably that of Mark).\n",
      " \n",
      "As it is assumed that John knew the content of Luke's text. The evidence\n",
      "for that is not overwhelming, admittedly.\n",
      " \n",
      " \n",
      ">>>(1)  Earlier manuscripts of John have been discovered.\n",
      ">\n",
      ">>Interesting, where and which? How are they dated? How old are they?\n",
      ">\n",
      ">Unfortunately, I haven't got the info at hand.  It was (I think) in the late\n",
      ">'70s or early '80s, and it was possibly as old as CE 200.\n",
      ">\n",
      " \n",
      "When they are from about 200, why do they shed doubt on the order on\n",
      "putting John after the rest of the three?\n",
      " \n",
      " \n",
      ">>I don't see your point, it is exactly what James Felder said.  They had no\n",
      ">>first hand knowledge of the events, and it obvious that at least two of them\n",
      ">>used older texts as the base of their account.  And even the association of\n",
      ">>Luke to Paul or Mark to Peter are not generally accepted.\n",
      ">\n",
      ">Well, a genuine letter of Peter would be close enough, wouldn't it?\n",
      ">\n",
      " \n",
      "Sure, an original together with Id card of sender and receiver would be\n",
      "fine. So what's that supposed to say? Am I missing something?\n",
      " \n",
      " \n",
      ">And I don't think a \"one step removed\" source is that bad.  If Luke and Mark\n",
      ">and Matthew learned their stories directly from diciples, then I really\n",
      ">cannot believe in the sort of \"big transformation from Jesus to gospel\" that\n",
      ">some people posit.  In news reports, one generally gets no better\n",
      ">information than this.\n",
      ">\n",
      ">And if John IS a diciple, then there's nothing more to be said.\n",
      ">\n",
      " \n",
      "That John was a disciple is not generally accepted. The style and language\n",
      "together with the theology are usually used as counterargument.\n",
      " \n",
      "The argument that John was a disciple relies on the claim in the gospel\n",
      "of John itself. Is there any other evidence for it?\n",
      " \n",
      "One step and one generation removed is bad even in our times. Compare that\n",
      "to reports of similar events in our century in almost illiterate societies.\n",
      "Not even to speak off that believers are not necessarily the best sources.\n",
      " \n",
      " \n",
      ">>It is also obvious that Mark has been edited. How old are the oldest\n",
      ">>manuscripts? To my knowledge (which can be antiquated) the oldest is\n",
      ">>quite after any of these estimates, and it is not even complete.\n",
      ">\n",
      ">The only clear \"editing\" is problem of the ending, and it's basically a\n",
      ">hopeless mess.  The oldest versions give a strong sense of incompleteness,\n",
      ">to the point where the shortest versions seem to break off in midsentence.\n",
      ">The most obvious solution is that at some point part of the text was lost.\n",
      ">The material from verse 9 on is pretty clearly later and seems to represent\n",
      ">a synopsys of the end of Luke.\n",
      ">\n",
      "In other words, one does not know what the original of Mark did look like\n",
      "and arguments based on Mark are pretty weak.\n",
      " \n",
      "But how is that connected to a redating of John?\n",
      "   Benedikt\n",
      "\n",
      "0\n",
      "LABEL: talk.politics.mideast ----END----\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's view random indices in the dataset\n",
    "\n",
    "import random\n",
    "\n",
    "samples = random.sample(range(len(trainset['data'])), 3)\n",
    "\n",
    "for idx in samples:\n",
    "    doc = trainset['data'][idx]\n",
    "    label = trainset['target'][idx]\n",
    "    \n",
    "    print(doc)\n",
    "    print(label)\n",
    "    print(f'LABEL: {trainset[\"target_names\"][label]} ----END----\\n\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can do any sort of training on the dataset, we first need to preprocess it to make it more meaningful and remove unnecessary context. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/prashanthjaganathan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/prashanthjaganathan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import download\n",
    "from typing import List, Dict, Tuple\n",
    "import numpy as np\n",
    "\n",
    "download('punkt')  # for word_tokenize\n",
    "download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below function preprocesses text by:\n",
    "\n",
    "- Converting to lowercase.\n",
    "- Removing emails, URLs, special characters, and unwanted words.\n",
    "- Removing stopwords and single-word or empty documents.\n",
    "- Tokenizing the text (white-space)\n",
    "- Lemmatizing words to their base form, and also removes numbers\n",
    "\n",
    "Lemmatization is used instead of stemming because it produces valid dictionary words and preserves meaning and context, which is important for tasks like text classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_document(document: str):\n",
    "    \"\"\"Remove unnecessary words from the document\"\"\"\n",
    "\n",
    "    # Convert to lower case\n",
    "    document = document.lower()\n",
    "\n",
    "    # Remove emails, URLs, etc.\n",
    "    document = re.sub(r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}', '', document)\n",
    "    document = re.sub(r'http\\S+', '', document)\n",
    "\n",
    "    # Remove stop words\n",
    "    words = document.strip().split()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stop_words.update(['forward', 'reply'])  # Add words to stop_words set\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    document = ' '.join(words)\n",
    "\n",
    "    # Remove special characters like -, <>, etc.\n",
    "    document = re.sub(r'[^a-zA-Z0-9\\s]', '', document)  # Remove non-alphanumeric characters except spaces\n",
    "\n",
    "    # Remove words like From, Subject, Originator, Lines, Nntp-Posting-Host, Organization\n",
    "    unwanted_words = ['from', 'subject', 'originator', 'lines', 'nntppostinghost', 'organization', 're']\n",
    "    document = ' '.join([word for word in document.split() if word not in unwanted_words])\n",
    "\n",
    "    # Tokenize the document (split into words)\n",
    "    words = word_tokenize(document)\n",
    "\n",
    "    # Add code to remove numbers here\n",
    "\n",
    "    # Perform lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word, pos=\"v\") for word in words if not word.isdigit()]\n",
    "\n",
    "    # Remove single word and empty docs\n",
    "    if len(lemmatized_words) <= 1:\n",
    "        return None  # return None if the document has no meaningful content\n",
    "\n",
    "    # Reconstruct the document\n",
    "    cleaned_document = ' '.join(lemmatized_words)\n",
    "\n",
    "    return lemmatized_words, cleaned_document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test our preprocessor on one document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mark prado sixtytwo thousand be many read scispace address anyone know anyone else would like get scispace internet fee or cryptic internet fee would will fee them nice offline message readereditor automate modem mailer program pick mail bundle quickly easily installexe set painlessly charge scispace fee though dial washington dc bbs store system mail bundle minimum connect time im use overseas call this offer free fee particular newsgroups speed v32bis support vips might offer free service internet address functionality get fee uunet run 4line hub hubbing years extremely reliable hub software provide run msdos and os2 windows do box other compatible software package exist macintosh unix responses private go to by way all apologies public traffic glib question really expect public reply thank bill higgins interest statistics lead origin permannet ftsc internet gateway'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document = \"\"\"From: Mark.Prado@p2.f349.n109.z1.permanet.org (Mark Prado)\n",
    "Subject: Sixty-two thousand (was Re: How many read sci.space?)\n",
    "Lines: 32\n",
    "\n",
    "\n",
    "Reply address: mark.prado@permanet.org\n",
    "\n",
    "If anyone knows anyone else who would like to get sci.space,\n",
    "but doesn't have an Internet feed (or has a cryptic Internet\n",
    "feed), I would be willing to feed it to them.  I have a nice\n",
    "offline message reader/editor, an automated modem \"mailer\"\n",
    "program which will pick up mail bundles (quickly and easily),\n",
    "and an INSTALL.EXE to set them up painlessly.  No charge for\n",
    "the sci.space feed, though you have to dial Washington, D.C.\n",
    "This is NOT a BBS -- it's a store & forward system for mail\n",
    "bundles, with minimum connect times.  (I'm used to overseas\n",
    "calls.)  (This is not an offer for a free feed for any other\n",
    "particular newsgroups.)  Speeds of up to 14400 (v32bis) are\n",
    "supported.  VIP's might be offered other free services, such\n",
    "as Internet address and other functionality.\n",
    "\n",
    "I get my feed from UUNET and run a 4-line hub.  I've been\n",
    "hubbing for years -- I have an extremely reliable hub.\n",
    "\n",
    "The software I provide runs under MS-DOS (and OS/2 and Windows\n",
    "as a DOS box).  Other, compatible software packages exist for\n",
    "the MacIntosh and Unix.\n",
    "\n",
    "Any responses should be private and go to:  \n",
    "mark.prado@permanet.org\n",
    "\n",
    "(By the way, to all, my apologies for the public traffic on my\n",
    "glib question.  I really didn't expect public replys.  But thanks\n",
    "to Bill Higgins for the interesting statistics and the lead.)\n",
    "\n",
    " * Origin: PerManNet FTSC <=> Internet gateway (1:109/349.2)\n",
    "\"\"\"\n",
    "\n",
    "tokenized_doc, document = preprocess_document(document)\n",
    "document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's preprocess the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset(documents, labels):\n",
    "    processed_labels = []\n",
    "    processed_tokenized_docs = []\n",
    "    processed_docs: List[str] = []\n",
    "    for document, label in zip(documents, labels):\n",
    "        tokenized_doc, document = preprocess_document(document)\n",
    "        if document is not None:\n",
    "            processed_docs.append(document)\n",
    "            processed_labels.append(label)\n",
    "            processed_tokenized_docs.append(tokenized_doc)\n",
    "\n",
    "    return processed_tokenized_docs, processed_docs, processed_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset['tokenized_data'], trainset['data'], trainset['target'] = preprocess_dataset(trainset['data'], trainset['target'])\n",
    "testset['tokenized_data'], testset['data'], testset['target'] = preprocess_dataset(testset['data'], testset['target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Representations\n",
    "\n",
    "Now that we have the dataset pre-processed, let's convert the word into embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_vectorized_train_corpus = tfidf_vectorizer.fit_transform(trainset['data'])\n",
    "tfidf_vectorized_test_corpus = tfidf_vectorizer.transform(testset['data'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trained Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "word2vec_vectorizer = Word2Vec(sentences=trainset['tokenized_data'], vector_size=300, window=10)\n",
    "\n",
    "def get_document_vector(words, model):\n",
    "    valid_words = [word for word in words if word in model.wv]\n",
    "    \n",
    "    if not valid_words:\n",
    "        return np.zeros(model.vector_size)\n",
    "    \n",
    "    word_vectors = np.array([model.wv[word] for word in valid_words])\n",
    "    document_vector = np.mean(word_vectors, axis=0)\n",
    "    # document_vector = np.linalg.norm(word_vectors, axis=0)\n",
    "    \n",
    "    return document_vector\n",
    "\n",
    "\n",
    "word2vec_vectorized_train_corpus = np.array([get_document_vector(doc, word2vec_vectorizer) for doc in trainset['tokenized_data']])\n",
    "word2vec_vectorized_test_corpus = np.array([get_document_vector(doc, word2vec_vectorizer) for doc in testset['tokenized_data']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-trained Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3.22478935e-02  2.51459666e-02  3.05642430e-02  8.23636875e-02\n",
      "  -4.10373919e-02 -5.55739179e-02  3.24003361e-02 -7.62371346e-02\n",
      "   9.44162011e-02  7.79554099e-02 -2.67080665e-02 -1.20082222e-01\n",
      "  -7.26899058e-02  6.18105382e-02 -1.09070711e-01  7.62318373e-02\n",
      "  -6.11698180e-02  9.55687612e-02  1.29045649e-02 -1.28017247e-01\n",
      "   1.44207412e-02  1.46352900e-02  7.40464032e-02  3.07931262e-03\n",
      "   2.36822339e-03 -3.83814834e-02 -3.44493836e-02  3.32995281e-02\n",
      "   8.54092166e-02 -7.39862770e-02  4.70619574e-02 -7.33743310e-02\n",
      "  -8.75527859e-02  1.66909676e-02 -7.54641965e-02 -2.76427045e-02\n",
      "   8.26629326e-02  7.36553594e-02  3.39817666e-02  4.08144444e-02\n",
      "   9.98479575e-02  7.26093352e-03  1.56373844e-01 -4.23527695e-02\n",
      "   3.28020267e-02 -5.08261919e-02 -1.83327682e-02 -3.50073650e-02\n",
      "  -7.87057206e-02  8.31426214e-03  2.51607061e-03  6.23758556e-03\n",
      "   3.80729027e-02  9.29526892e-03 -6.60566315e-02 -1.84292104e-02\n",
      "  -8.05564821e-02 -2.59923842e-02 -4.06723022e-02 -1.09897055e-01\n",
      "  -6.17178008e-02  6.24653362e-02 -3.56361605e-02 -2.34347600e-02\n",
      "  -2.14400794e-02 -4.20236662e-02 -3.86512540e-02  5.78943640e-02\n",
      "  -1.14227958e-01  5.24035692e-02 -1.88284572e-02  8.02785605e-02\n",
      "   3.18302773e-02  4.20861086e-03 -1.46848217e-01 -1.16887212e-01\n",
      "   5.64181507e-02  2.00771596e-02  1.64090134e-02  5.88931479e-02\n",
      "  -2.87320036e-02  2.19546948e-02 -3.06449807e-03 -1.21344405e-03\n",
      "  -2.66335495e-02 -8.57106876e-03 -6.99622855e-02  9.68109444e-02\n",
      "   9.07749811e-04  3.67899775e-03 -3.42400558e-03 -2.39384472e-02\n",
      "  -1.29821628e-01 -5.60675859e-02 -4.28838655e-02 -4.69037378e-03\n",
      "   6.78553134e-02 -1.63197927e-02  7.22156987e-02 -5.66560309e-03\n",
      "  -4.60513234e-02 -2.11246815e-02  5.23428693e-02  2.94126496e-02\n",
      "   3.09360828e-02 -5.26360087e-02 -4.03694771e-02  8.98090843e-03\n",
      "   6.43674955e-02 -5.42974025e-02 -5.73436581e-02 -5.07027321e-02\n",
      "   4.05528247e-02  2.16046683e-02 -8.71958304e-03 -5.90814389e-02\n",
      "   1.43542243e-02 -2.51244102e-02  3.54532786e-02  7.16200173e-02\n",
      "  -6.31310940e-02  4.08955552e-02 -8.35425705e-02  1.32770492e-02\n",
      "   1.67278554e-02 -4.33925129e-02 -3.37872542e-02 -7.11469948e-02\n",
      "   3.07442378e-02  2.34879423e-02 -1.92209631e-02 -8.56708437e-02\n",
      "  -1.17800077e-02  3.24789062e-02  4.11002152e-03 -1.43591873e-02\n",
      "  -9.13801510e-03 -4.35220674e-02  2.26884112e-02  2.72575673e-02\n",
      "   5.82863502e-02 -5.92239685e-02  3.97898853e-02 -1.27551626e-04\n",
      "  -5.73611935e-04 -5.38798189e-03 -5.16610742e-02 -1.57665890e-02\n",
      "  -6.20265342e-02 -6.27934709e-02  1.90456361e-01 -4.20059636e-02\n",
      "  -6.65640235e-02  4.65989346e-03 -1.91888344e-02 -1.01666793e-01\n",
      "  -4.59049568e-02 -2.83539407e-02 -3.21174562e-02 -4.58210707e-02\n",
      "  -3.56600396e-02  8.24300498e-02  1.34309931e-02 -8.67431015e-02\n",
      "   3.26301064e-03 -1.28407598e-01  9.20933113e-02 -4.28116433e-02\n",
      "   9.49274469e-03  2.55608428e-02 -5.93093224e-02  2.14358959e-02\n",
      "  -3.89934555e-02 -5.94503172e-02  1.31990006e-02 -4.40499000e-02\n",
      "   8.53124857e-02 -9.37953293e-02  2.78746965e-03 -7.99670145e-02\n",
      "  -1.02956533e-01 -5.76072447e-02 -3.41919847e-02 -9.04238783e-03\n",
      "  -2.95442739e-03  1.28502706e-02  2.26507094e-02  1.11230649e-02\n",
      "   1.07838236e-01  9.01686996e-02  1.90601535e-02  8.01677555e-02\n",
      "   2.24398505e-02  5.18693263e-03 -2.07578787e-03  6.39556199e-02\n",
      "  -1.96023588e-03  2.26720143e-02 -2.47710887e-02 -5.43455854e-02\n",
      "  -2.72931587e-02  4.74000201e-02 -3.91266458e-02 -4.63744029e-02\n",
      "   1.40371975e-02 -6.12686314e-02 -1.13575459e-02 -8.27755034e-03\n",
      "  -2.48522721e-02 -3.65286507e-02  2.56575793e-02  5.13145104e-02\n",
      "  -3.81034203e-02  4.21068491e-03 -1.05300754e-01 -3.26523278e-03\n",
      "   2.41959579e-02  6.98200718e-04 -1.03790805e-01 -3.92875299e-02\n",
      "  -3.39151025e-02 -2.59079430e-02  3.70083414e-02  3.57617140e-02\n",
      "   2.75713727e-02 -8.78689960e-02  8.59101638e-02  5.40149286e-02\n",
      "   9.74592194e-02 -4.15228307e-02  7.33212978e-02 -6.68680146e-02\n",
      "  -1.75978837e-03  4.71079648e-02  6.83893710e-02 -3.34569328e-02\n",
      "   4.66592275e-02  3.96955907e-02  7.60783255e-02 -1.54558197e-03\n",
      "   8.97485688e-02 -4.53208253e-04 -5.11865690e-03 -2.32065450e-02\n",
      "  -1.59772858e-02  9.99584720e-02 -1.47925811e-02  2.35756245e-02\n",
      "  -5.38096018e-02 -9.42776538e-03  2.03944836e-02  2.03823354e-02\n",
      "   1.65540092e-02  6.91447556e-02  1.08028673e-01 -9.16056558e-02\n",
      "   4.94764931e-02 -2.78167725e-02  3.62459943e-02 -7.60989860e-02\n",
      "  -4.37705033e-03 -1.72241908e-02 -9.93406400e-03  6.31721271e-03\n",
      "   2.61491202e-02  9.05897990e-02 -7.22631067e-02 -2.91235466e-02\n",
      "  -1.20001040e-01 -3.74801420e-02 -2.14793384e-02  6.11206368e-02\n",
      "   1.72753949e-02 -3.28138034e-04  7.23669529e-02 -5.28752580e-02\n",
      "  -5.38111739e-02 -2.16318145e-02 -4.04221602e-02  6.06686473e-02\n",
      "  -5.22887614e-03  1.98396854e-02  1.86151303e-02  5.81433922e-02\n",
      "   8.66392441e-03 -5.24413493e-03 -9.37417075e-02  3.58590446e-02\n",
      "   4.25369851e-02  3.11722253e-02 -4.65226397e-02  4.55043763e-02\n",
      "  -1.29139274e-01 -6.42224634e-03 -8.40187073e-02  2.81680212e-03\n",
      "  -9.37612578e-02 -6.40809909e-02  3.75564732e-02  6.64672852e-02]\n",
      " [ 3.19081843e-02  5.03175717e-04  3.19987088e-02  1.21743880e-01\n",
      "  -1.13000579e-01  1.09112347e-02  7.42797852e-02 -4.22851890e-02\n",
      "   8.28388110e-02  7.32463226e-02 -5.90788610e-02 -1.17466532e-01\n",
      "  -1.17992617e-01  6.38311580e-02 -1.10997148e-01  1.22339144e-01\n",
      "   6.81819245e-02  9.99246687e-02  5.04277274e-03 -9.77061391e-02\n",
      "   1.86674353e-02  9.82117373e-03  5.32763824e-02 -3.94252837e-02\n",
      "   5.65466704e-03 -8.39536637e-03 -5.54513820e-02  2.60215495e-02\n",
      "   1.09077413e-02 -2.42499877e-02 -2.68400385e-04 -1.54189207e-02\n",
      "  -9.60830525e-02 -1.38870412e-02 -4.38045561e-02  1.73135828e-02\n",
      "   3.74789275e-02  3.08508277e-02  5.85748702e-02  7.61956647e-02\n",
      "   6.95149302e-02 -2.47259252e-02  1.80944383e-01 -2.90407334e-02\n",
      "  -2.44373102e-02 -5.51435500e-02 -3.23108286e-02 -1.25358663e-02\n",
      "  -1.04556993e-01  1.89148113e-02 -4.89208773e-02  3.58017497e-02\n",
      "  -1.32414568e-02  1.09750126e-02 -1.78956450e-03  1.23951090e-02\n",
      "  -6.49964437e-02 -5.14486916e-02  3.70407738e-02 -7.18824342e-02\n",
      "  -1.80900656e-02  4.60757986e-02 -2.91837193e-02 -7.06991106e-02\n",
      "   7.85154663e-03 -9.56918076e-02 -6.18070960e-02  7.94586465e-02\n",
      "  -7.92729259e-02  4.83891331e-02  3.31420898e-02  9.02226493e-02\n",
      "   5.38623258e-02  1.78548414e-02 -1.39894292e-01 -9.21703726e-02\n",
      "   4.30379286e-02  7.76507780e-02  1.68259870e-02  8.31341669e-02\n",
      "  -4.42684256e-02  1.61357410e-02 -2.25985777e-02  2.09731190e-03\n",
      "  -2.13285293e-02  3.35342018e-03 -9.63854864e-02  1.40172467e-01\n",
      "   9.12262313e-03 -3.31424316e-03  2.05234140e-02 -1.34069463e-02\n",
      "  -1.00530557e-01 -4.73477878e-02  7.00537022e-03 -3.16872746e-02\n",
      "   7.94810951e-02  1.88373178e-02  3.39712948e-02  7.90846720e-03\n",
      "  -4.18125093e-02 -1.04914466e-02  2.40072180e-02  2.88817156e-02\n",
      "  -3.73681751e-03 -4.29559574e-02 -3.57211679e-02  6.53144764e-03\n",
      "   2.52126623e-02 -6.23921603e-02 -8.97261351e-02 -6.83794543e-02\n",
      "  -2.27057636e-02  2.52802130e-02  6.41245916e-02 -1.70872509e-02\n",
      "   5.25107235e-02 -4.22488451e-02  4.74446528e-02  6.61583394e-02\n",
      "  -9.49259549e-02  1.75150316e-02 -5.83153218e-02  5.01164198e-02\n",
      "  -2.36596959e-03 -3.73292454e-02 -6.43399730e-02  1.58485665e-03\n",
      "   1.44399581e-02 -1.84012428e-02 -3.46000753e-02 -7.52143413e-02\n",
      "  -6.40073642e-02  7.21492106e-03 -8.84542614e-03 -5.70402667e-02\n",
      "  -1.62434950e-02 -1.85178907e-03  4.79154252e-02  3.78475823e-02\n",
      "   9.15054604e-02 -9.82594043e-02  1.85684040e-02  5.62569266e-03\n",
      "   5.09945303e-02  2.49678362e-03 -2.09840927e-02 -5.68612777e-02\n",
      "  -3.30707692e-02 -4.02218252e-02  1.47669509e-01 -1.70486961e-02\n",
      "  -6.06593452e-02  9.47968382e-03 -3.56666483e-02 -4.46863063e-02\n",
      "  -7.71705508e-02 -2.19627116e-02 -4.11709547e-02 -4.25113291e-02\n",
      "   2.24083029e-02  5.07372729e-02 -2.10216399e-02 -5.45809455e-02\n",
      "   2.18433850e-02 -8.10002089e-02  7.64269903e-02 -6.09233715e-02\n",
      "  -1.87875126e-02  6.99298317e-03 -1.01574503e-01 -1.98664721e-02\n",
      "  -2.07732990e-02 -7.40841627e-02 -2.96599995e-02 -4.37224321e-02\n",
      "   1.15628317e-01 -9.72702280e-02  2.28955559e-02 -5.02768755e-02\n",
      "  -6.73965290e-02 -6.80372715e-02 -2.00915392e-02 -2.75742612e-03\n",
      "   3.04061366e-04  2.77803391e-02 -1.41342673e-02 -2.77421921e-02\n",
      "   5.73385656e-02  8.55358019e-02  5.01595847e-02  2.27287374e-02\n",
      "   3.40188704e-02  2.31742207e-02  2.86197439e-02  3.95835266e-02\n",
      "  -6.46561198e-03 -8.41104332e-03 -6.37397319e-02 -7.96372369e-02\n",
      "  -3.55650671e-02  7.31931552e-02 -2.71400809e-02 -4.19530123e-02\n",
      "   6.70700916e-04 -3.57976332e-02  3.99471668e-04 -5.36385104e-02\n",
      "  -5.96256889e-02  9.72559210e-03  7.45434687e-03  8.25156942e-02\n",
      "  -2.70186644e-02 -9.95155890e-03 -1.27047077e-01  8.39919201e-04\n",
      "   6.95435628e-02 -1.76795367e-02 -5.63651510e-02 -6.60095885e-02\n",
      "  -2.26167832e-02  4.28857701e-03  2.18934901e-02  3.16718258e-02\n",
      "   8.30069557e-02 -3.13811563e-02  1.09142393e-01  6.13750517e-02\n",
      "   1.11269146e-04 -7.91656878e-03  5.01344875e-02 -4.45296057e-02\n",
      "  -1.32886907e-02  3.12902890e-02  4.21135947e-02 -1.82862021e-02\n",
      "   5.10401353e-02  1.63262188e-02  9.26195607e-02 -4.16860916e-02\n",
      "   7.09949806e-02 -2.27184501e-03  6.70958124e-03 -3.95307206e-02\n",
      "  -5.87675981e-02  7.19024986e-02  1.14687802e-02  1.09547198e-01\n",
      "  -1.88167226e-02 -4.64472808e-02  4.97985166e-03  3.28667462e-02\n",
      "   2.14205962e-02  6.22944348e-02  6.07550107e-02 -8.16804692e-02\n",
      "   2.64446810e-02  4.58795577e-02  9.07300040e-03 -7.26537779e-02\n",
      "   2.27488833e-03  1.01407515e-02 -3.90990190e-02  1.51881529e-02\n",
      "   8.47462844e-03  1.10412084e-01 -6.54778630e-02 -3.32963057e-02\n",
      "  -9.47503969e-02 -6.61480427e-02 -2.42781043e-02  7.29467869e-02\n",
      "   9.07841399e-02  3.59443910e-02  1.75552368e-02 -8.33543092e-02\n",
      "  -5.06636575e-02 -7.18390644e-02 -8.24519843e-02  7.07862750e-02\n",
      "  -2.84244660e-02 -1.61090698e-02  3.43873538e-02  7.51750842e-02\n",
      "  -3.42551470e-02 -9.73302871e-03 -5.51461205e-02 -1.02779092e-02\n",
      "   1.97172165e-02  1.43903242e-02 -5.84583078e-03  9.70928743e-02\n",
      "  -7.65560418e-02  5.41601814e-02 -6.05200902e-02 -6.24050610e-02\n",
      "  -3.11074145e-02 -8.09202716e-02  9.82421730e-03  3.59011889e-02]\n",
      " [ 4.30158190e-02  2.73015630e-02  3.68093699e-02  1.27179682e-01\n",
      "  -4.96487357e-02  2.71275416e-02  3.95487137e-02 -4.93870117e-02\n",
      "   6.89911917e-02  4.40027602e-02 -3.62021253e-02 -1.15052432e-01\n",
      "  -5.79887666e-02  5.53407818e-02 -8.52004215e-02  1.19227521e-01\n",
      "   3.91514674e-02  1.02907345e-01  1.69331171e-02 -6.45974353e-02\n",
      "   1.44044263e-02  3.22007015e-02  7.11243227e-02 -2.82738898e-02\n",
      "   1.67103615e-02 -2.66775358e-02 -5.79404682e-02  6.11427426e-02\n",
      "   5.27116172e-02 -3.56825478e-02 -2.15121768e-02  1.36491163e-02\n",
      "  -7.19588995e-02 -8.63621570e-03 -3.80533524e-02 -1.18662948e-02\n",
      "   4.35554385e-02  3.27676013e-02  7.79483616e-02  9.76367220e-02\n",
      "   5.14513440e-02 -5.52585423e-02  1.23860762e-01  1.73947606e-02\n",
      "   1.95578877e-02 -6.17693886e-02 -1.96926314e-02 -3.15260813e-02\n",
      "  -1.84781346e-02  2.12588557e-03 -7.39585757e-02  3.13711651e-02\n",
      "   2.92989444e-02 -1.55262062e-02  1.10730967e-02  3.75262753e-04\n",
      "  -5.12315147e-02 -6.23793527e-02 -5.57954423e-03 -7.62009695e-02\n",
      "  -1.50524070e-02  1.60736479e-02 -7.09140599e-02 -3.56584974e-02\n",
      "  -1.49975792e-02 -7.33343884e-02 -7.69758075e-02  4.90526110e-02\n",
      "  -3.24906185e-02  8.77970010e-02  9.72630456e-02  3.58167738e-02\n",
      "   3.56064960e-02 -1.52285304e-02 -1.22416481e-01 -5.74114993e-02\n",
      "   4.67244796e-02  3.33738178e-02  3.07982173e-02  6.98754266e-02\n",
      "  -2.94313603e-03 -2.58064922e-02  6.37752749e-03 -3.69402356e-02\n",
      "  -2.38792282e-02 -3.00106760e-02 -8.74654502e-02  1.18760429e-01\n",
      "   3.81340422e-02  1.33864842e-02  7.14644119e-02  2.57193353e-02\n",
      "  -7.69355893e-02 -9.96683389e-02 -8.05767532e-03 -1.07390843e-02\n",
      "   1.30266091e-02  6.91936910e-02  6.93307668e-02 -9.83087569e-02\n",
      "  -6.32065907e-02  2.55724378e-02  5.64849339e-02 -1.07544726e-02\n",
      "  -3.79767977e-02 -3.81573178e-02 -4.19152156e-02  2.22426597e-02\n",
      "   4.47540283e-02 -3.97560000e-02 -1.11599162e-01 -2.04215609e-02\n",
      "   1.32185081e-02  2.14365292e-02  1.20962888e-01 -6.98351441e-03\n",
      "   1.98499709e-02 -6.80080503e-02  1.02547243e-01  3.61399576e-02\n",
      "  -4.68781032e-02 -1.59027614e-02 -8.58009458e-02  4.27846089e-02\n",
      "   6.93421438e-03 -3.56577225e-02 -6.40501902e-02 -3.64193670e-03\n",
      "   4.94289063e-02  9.79252160e-03 -1.07392907e-01 -1.03203982e-01\n",
      "  -8.19215924e-02  2.46380307e-02 -1.05888080e-02 -5.47402585e-03\n",
      "  -3.44590023e-02 -1.99476983e-02  3.93286236e-02  7.29233027e-02\n",
      "   8.88973176e-02 -4.63856831e-02 -4.20739129e-02 -3.37076671e-02\n",
      "   2.17181873e-02  3.03815417e-02 -5.54902665e-02 -6.78294525e-02\n",
      "  -4.47981246e-02 -7.11041465e-02  1.08247921e-01 -3.56279803e-03\n",
      "  -9.44128558e-02  1.00100245e-02 -1.84688251e-02 -1.07613772e-01\n",
      "  -9.79148690e-03 -2.25832667e-02 -4.23894338e-02 -5.69095947e-02\n",
      "  -9.16510075e-03  5.47146536e-02  3.86662893e-02 -4.72375890e-03\n",
      "   3.95135395e-02 -1.32354736e-01  6.22074008e-02 -2.03422289e-02\n",
      "  -8.75952765e-02  6.33317325e-03 -6.44645020e-02 -2.19733026e-02\n",
      "  -1.38866622e-02 -6.32823333e-02 -3.24873850e-02  3.01689543e-02\n",
      "   7.39166811e-02 -8.42830837e-02  1.66025963e-02 -6.48772642e-02\n",
      "  -6.14633076e-02 -5.91073781e-02  3.10924985e-02  1.41844666e-02\n",
      "  -5.90106510e-02  2.13012695e-02 -4.31063361e-02  3.20281982e-02\n",
      "   7.54464343e-02  4.88534719e-02  1.62208695e-02  5.07428274e-02\n",
      "   1.39522227e-02 -1.69157907e-02  2.00888421e-02  4.38079834e-02\n",
      "   5.83092635e-03 -5.82168065e-03 -1.01924576e-01 -7.16811344e-02\n",
      "  -7.76098222e-02  5.61830886e-02 -7.94667378e-02 -1.90631412e-02\n",
      "   5.79058134e-04 -7.84746557e-02 -5.54314964e-02  1.81724168e-02\n",
      "  -4.13797684e-02 -9.08292364e-03 -4.90701981e-02  5.79409860e-02\n",
      "  -3.42859812e-02 -6.33058697e-03 -1.27527267e-01  3.74284536e-02\n",
      "   9.53640342e-02 -2.68179681e-02 -1.02931522e-01 -4.09038998e-02\n",
      "   4.71892580e-03  4.67987061e-02  7.34542590e-03 -1.65286306e-02\n",
      "   8.13133121e-02 -3.78443822e-02  7.14452714e-02  3.93387116e-02\n",
      "   2.87935939e-02 -4.92875688e-02  4.99870181e-02 -3.69492881e-02\n",
      "  -2.50560623e-02 -3.66428196e-02  8.02176520e-02  1.25631560e-02\n",
      "   2.93341819e-02 -2.19149832e-02  5.58370799e-02 -1.16151711e-02\n",
      "   7.00416416e-02 -2.88292840e-02 -7.33714970e-03 -6.76672980e-02\n",
      "  -3.65273431e-02  3.77461053e-02  2.78278925e-02  7.19366595e-02\n",
      "  -4.34633344e-02 -3.09335086e-02  2.98182592e-02  9.38725844e-02\n",
      "   4.25133780e-02  8.21297839e-02  4.52560149e-02 -8.33247527e-02\n",
      "  -5.13146305e-03  1.36289112e-02 -7.61170462e-02 -3.85545641e-02\n",
      "   4.14211489e-03 -3.85969765e-02 -2.58596390e-02 -6.65386673e-03\n",
      "   3.83670628e-02  5.54288775e-02 -2.89057717e-02 -3.88328433e-02\n",
      "  -6.93197772e-02 -5.47079965e-02  1.58594064e-02  3.48113738e-02\n",
      "   8.65884572e-02  4.30697426e-02  6.98984414e-02 -6.85497224e-02\n",
      "  -9.00746360e-02 -8.65659565e-02 -3.22720632e-02  6.01163954e-02\n",
      "   2.10235082e-03 -9.00035817e-03  2.44699251e-02  5.50676771e-02\n",
      "   5.87897860e-02  1.73598472e-02 -4.15875055e-02  2.44719945e-02\n",
      "  -3.88135738e-03  4.31299061e-02  4.61479183e-03  5.43435290e-02\n",
      "  -8.84171799e-02  2.52620894e-02 -5.55254407e-02 -4.12387699e-02\n",
      "  -7.32266717e-03 -6.42209649e-02 -3.48681435e-02 -1.00687314e-02]]\n",
      "(7532, 300)\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import numpy as np\n",
    "\n",
    "model_path = 'GoogleNews-vectors-negative300.bin'\n",
    "\n",
    "word2vec_word_embeddings = gensim.models.KeyedVectors.load_word2vec_format(model_path, binary=True)\n",
    "words = word2vec_word_embeddings.index_to_key\n",
    "pretrained_word2vec_word_embeddings = {}\n",
    "for word in words:\n",
    "    pretrained_word2vec_word_embeddings[word] = word2vec_word_embeddings[word]\n",
    "\n",
    "# Function to get document vectors using pretrained embeddings\n",
    "def get_document_vector_with_pretrained_embeddings(words, pretrained_model):\n",
    "    valid_words = [word for word in words if word in pretrained_model]\n",
    "    \n",
    "    if not valid_words:\n",
    "        return np.zeros(len(pretrained_model['the']))  # Default to zero vector (length of the word vector)\n",
    "    \n",
    "    word_vectors = np.array([pretrained_model[word] for word in valid_words])\n",
    "    document_vector = np.mean(word_vectors, axis=0)\n",
    "    # document_vector = np.max(word_vectors, axis=0)\n",
    "    # document_vector = np.linalg.norm(word_vectors, axis=0)\n",
    "    # document_vector = np.sum(word_vectors, axis=0)\n",
    "\n",
    "    return document_vector\n",
    "\n",
    "pretrained_word2vec_vectorized_train_corpus = np.array([get_document_vector_with_pretrained_embeddings(doc, pretrained_word2vec_word_embeddings) for doc in trainset['tokenized_data']])\n",
    "pretrained_word2vec_vectorized_test_corpus = np.array([get_document_vector_with_pretrained_embeddings(doc, pretrained_word2vec_word_embeddings) for doc in testset['tokenized_data']])\n",
    "\n",
    "print(pretrained_word2vec_vectorized_train_corpus[:3])\n",
    "print(pretrained_word2vec_vectorized_test_corpus.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define and train the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, X_train, Y_train):\n",
    "    model.fit(X_train, Y_train)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Naive Bayes Classifier \n",
    "\n",
    "**NOTE:** We won't be able to use word2vec embeddings on MultinomialNB because it can take negative values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "naive_bayes_classifier_tfidf = MultinomialNB()\n",
    "naive_bayes_classifier_tfidf = train(naive_bayes_classifier_tfidf, tfidf_vectorized_train_corpus, trainset['target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# using tf-idf\n",
    "logistic_regression_clf_tfidf = LogisticRegression()\n",
    "logistic_regression_clf_tfidf = train(logistic_regression_clf_tfidf, tfidf_vectorized_train_corpus, trainset['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/prashanthjaganathan/miniconda3/envs/venv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# using word2vec\n",
    "logistic_regression_clf_word2vec = LogisticRegression()\n",
    "logistic_regression_clf_word2vec = train(logistic_regression_clf_word2vec, word2vec_vectorized_train_corpus, trainset['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# using pre-trained word2vec\n",
    "logistic_regression_clf_pretrained_word2vec = LogisticRegression()\n",
    "logistic_regression_clf_pretrained_word2vec = train(logistic_regression_clf_pretrained_word2vec, pretrained_word2vec_vectorized_train_corpus, trainset['target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv1D, MaxPooling1D, Embedding, GlobalMaxPool1D, Dropout, BatchNormalization, Input, Reshape\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from tensorflow.keras.metrics import Precision, Recall\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam, SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn_model(input_shape, num_classes=20):\n",
    "    \"\"\"Create a CNN model for text classification with improvements\"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=input_shape))\n",
    "    model.add(Reshape((input_shape[0], 1)))\n",
    "\n",
    "\n",
    "    # 4 convolutional layers with different filter sizes\n",
    "    model.add(Conv1D(256, 5, padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling1D())\n",
    "    \n",
    "    model.add(Conv1D(128, 5, padding='same', activation='relu'))  \n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling1D())\n",
    "    \n",
    "    model.add(Conv1D(64, 3, padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling1D())\n",
    "    \n",
    "    model.add(Conv1D(32, 3, padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling1D())\n",
    "\n",
    "    model.add(GlobalMaxPool1D())\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(Dense(200, activation='relu'))\n",
    "    model.add(Dropout(0.5))  # Dropout layer to prevent overfitting\n",
    "\n",
    "\n",
    "    model.add(Dense(num_classes, activation='softmax'))  # Softmax for multi-class classification\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=SGD(learning_rate=0.01, momentum=0.9), metrics=['accuracy', Precision(), Recall()])\n",
    "    \n",
    "    return model\n",
    "\n",
    "lr_scheduler = ReduceLROnPlateau(monitor='loss', factor=0.1, patience=3, verbose=1, min_lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2032"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxlen = max(tfidf_vectorized_test_corpus.getnnz(axis=1))\n",
    "maxlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorized_train_corpus_padded = pad_sequences(tfidf_vectorized_train_corpus.toarray(), maxlen)\n",
    "tfidf_vectorized_test_corpus_padded = pad_sequences(tfidf_vectorized_test_corpus.toarray(), maxlen)\n",
    "\n",
    "tfidf_vectorized_train_corpus_padded = tfidf_vectorized_train_corpus_padded.reshape(tfidf_vectorized_train_corpus_padded.shape[0], tfidf_vectorized_train_corpus_padded.shape[1], 1)\n",
    "tfidf_vectorized_test_corpus_padded = tfidf_vectorized_test_corpus_padded.reshape(tfidf_vectorized_test_corpus_padded.shape[0], tfidf_vectorized_test_corpus_padded.shape[1], 1)\n",
    "\n",
    "\n",
    "\n",
    "# Convert the target labels to one-hot encoding\n",
    "trainset_target = to_categorical(trainset['target'], num_classes=20)\n",
    "testset_target = to_categorical(testset['target'], num_classes=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tfidf_cnn = create_cnn_model(input_shape=(maxlen, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train target shape: (11314, 20)\n",
      "Test target shape: (7532, 20)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train target shape: {trainset_target.shape}\")\n",
    "print(f\"Test target shape: {testset_target.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "89/89 - 89s - 997ms/step - accuracy: 0.0488 - loss: 2.9946 - precision_8: 0.0000e+00 - recall_8: 0.0000e+00 - learning_rate: 0.0100\n",
      "Epoch 2/5\n",
      "89/89 - 92s - 1s/step - accuracy: 0.0508 - loss: 2.9924 - precision_8: 0.0000e+00 - recall_8: 0.0000e+00 - learning_rate: 0.0100\n",
      "Epoch 3/5\n",
      "89/89 - 93s - 1s/step - accuracy: 0.0483 - loss: 2.9914 - precision_8: 0.0000e+00 - recall_8: 0.0000e+00 - learning_rate: 0.0100\n",
      "Epoch 4/5\n",
      "89/89 - 94s - 1s/step - accuracy: 0.0492 - loss: 2.9910 - precision_8: 0.0000e+00 - recall_8: 0.0000e+00 - learning_rate: 0.0100\n",
      "Epoch 5/5\n",
      "89/89 - 96s - 1s/step - accuracy: 0.0492 - loss: 2.9908 - precision_8: 0.0000e+00 - recall_8: 0.0000e+00 - learning_rate: 0.0100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x310db9190>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_cnn.fit(tfidf_vectorized_train_corpus_padded, trainset_target, epochs=5, batch_size=128, verbose=2, callbacks=[lr_scheduler])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNN + Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_cnn = create_cnn_model(input_shape=(maxlen, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11314, 300)\n",
      "Target labels shape: (11314,)\n"
     ]
    }
   ],
   "source": [
    "print(word2vec_vectorized_train_corpus.shape)\n",
    "print(f\"Target labels shape: {np.array(trainset['target']).shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11314, 300)\n"
     ]
    }
   ],
   "source": [
    "# Assuming word2vec_vectorized_train_corpus is (num_samples, embedding_dim)\n",
    "# Reshape to (num_samples, sequence_length, embedding_dim), where sequence_length is known\n",
    "word2vec_vectorized_train_corpus = word2vec_vectorized_train_corpus.reshape(\n",
    "    word2vec_vectorized_train_corpus.shape[0],  # number of samples\n",
    "    maxlen, # the number of words in each document     # word2vec embedding dimension (e.g., 300)\n",
    ")\n",
    "\n",
    "print(word2vec_vectorized_train_corpus.shape)  # Should be (num_samples, sequence_length, embedding_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "89/89 - 14s - 154ms/step - accuracy: 0.2295 - loss: 2.4275 - precision_9: 0.5028 - recall_9: 0.0399\n",
      "Epoch 2/30\n",
      "89/89 - 13s - 148ms/step - accuracy: 0.3760 - loss: 1.8183 - precision_9: 0.6238 - recall_9: 0.1523\n",
      "Epoch 3/30\n",
      "89/89 - 13s - 149ms/step - accuracy: 0.4255 - loss: 1.6766 - precision_9: 0.6589 - recall_9: 0.2091\n",
      "Epoch 4/30\n",
      "89/89 - 14s - 155ms/step - accuracy: 0.4386 - loss: 1.6126 - precision_9: 0.6585 - recall_9: 0.2337\n",
      "Epoch 5/30\n",
      "89/89 - 15s - 164ms/step - accuracy: 0.4646 - loss: 1.5601 - precision_9: 0.6726 - recall_9: 0.2614\n",
      "Epoch 6/30\n",
      "89/89 - 14s - 161ms/step - accuracy: 0.4831 - loss: 1.5057 - precision_9: 0.6949 - recall_9: 0.2907\n",
      "Epoch 7/30\n",
      "89/89 - 14s - 160ms/step - accuracy: 0.4906 - loss: 1.4815 - precision_9: 0.6843 - recall_9: 0.3010\n",
      "Epoch 8/30\n",
      "89/89 - 14s - 163ms/step - accuracy: 0.5079 - loss: 1.4296 - precision_9: 0.7110 - recall_9: 0.3242\n",
      "Epoch 9/30\n",
      "89/89 - 15s - 165ms/step - accuracy: 0.5095 - loss: 1.4084 - precision_9: 0.7046 - recall_9: 0.3318\n",
      "Epoch 10/30\n",
      "89/89 - 14s - 162ms/step - accuracy: 0.5280 - loss: 1.3701 - precision_9: 0.7203 - recall_9: 0.3520\n",
      "Epoch 11/30\n",
      "89/89 - 14s - 161ms/step - accuracy: 0.5367 - loss: 1.3496 - precision_9: 0.7242 - recall_9: 0.3656\n",
      "Epoch 12/30\n",
      "89/89 - 14s - 156ms/step - accuracy: 0.5413 - loss: 1.3345 - precision_9: 0.7255 - recall_9: 0.3707\n",
      "Epoch 13/30\n",
      "89/89 - 14s - 156ms/step - accuracy: 0.5486 - loss: 1.3096 - precision_9: 0.7199 - recall_9: 0.3775\n",
      "Epoch 14/30\n",
      "89/89 - 14s - 157ms/step - accuracy: 0.5674 - loss: 1.2738 - precision_9: 0.7341 - recall_9: 0.4028\n",
      "Epoch 15/30\n",
      "89/89 - 14s - 156ms/step - accuracy: 0.5638 - loss: 1.2741 - precision_9: 0.7402 - recall_9: 0.4022\n",
      "Epoch 16/30\n",
      "89/89 - 14s - 158ms/step - accuracy: 0.5750 - loss: 1.2334 - precision_9: 0.7359 - recall_9: 0.4214\n",
      "Epoch 17/30\n",
      "89/89 - 14s - 159ms/step - accuracy: 0.5822 - loss: 1.2105 - precision_9: 0.7486 - recall_9: 0.4269\n",
      "Epoch 18/30\n",
      "89/89 - 14s - 159ms/step - accuracy: 0.5925 - loss: 1.1992 - precision_9: 0.7482 - recall_9: 0.4373\n",
      "Epoch 19/30\n",
      "89/89 - 14s - 158ms/step - accuracy: 0.5883 - loss: 1.1826 - precision_9: 0.7474 - recall_9: 0.4433\n",
      "Epoch 20/30\n",
      "89/89 - 14s - 159ms/step - accuracy: 0.6054 - loss: 1.1461 - precision_9: 0.7572 - recall_9: 0.4597\n",
      "Epoch 21/30\n",
      "89/89 - 14s - 158ms/step - accuracy: 0.6106 - loss: 1.1341 - precision_9: 0.7554 - recall_9: 0.4636\n",
      "Epoch 22/30\n",
      "89/89 - 14s - 158ms/step - accuracy: 0.6134 - loss: 1.1136 - precision_9: 0.7601 - recall_9: 0.4754\n",
      "Epoch 23/30\n",
      "89/89 - 14s - 157ms/step - accuracy: 0.6170 - loss: 1.0986 - precision_9: 0.7606 - recall_9: 0.4823\n",
      "Epoch 24/30\n",
      "89/89 - 14s - 155ms/step - accuracy: 0.6222 - loss: 1.0951 - precision_9: 0.7638 - recall_9: 0.4854\n",
      "Epoch 25/30\n",
      "89/89 - 14s - 157ms/step - accuracy: 0.6264 - loss: 1.0675 - precision_9: 0.7618 - recall_9: 0.4938\n",
      "Epoch 26/30\n",
      "89/89 - 14s - 158ms/step - accuracy: 0.6364 - loss: 1.0527 - precision_9: 0.7661 - recall_9: 0.5053\n",
      "Epoch 27/30\n",
      "89/89 - 14s - 162ms/step - accuracy: 0.6332 - loss: 1.0428 - precision_9: 0.7732 - recall_9: 0.5072\n",
      "Epoch 28/30\n",
      "89/89 - 13s - 148ms/step - accuracy: 0.6491 - loss: 1.0021 - precision_9: 0.7755 - recall_9: 0.5263\n",
      "Epoch 29/30\n",
      "89/89 - 13s - 150ms/step - accuracy: 0.6528 - loss: 0.9961 - precision_9: 0.7794 - recall_9: 0.5309\n",
      "Epoch 30/30\n",
      "89/89 - 13s - 148ms/step - accuracy: 0.6529 - loss: 0.9926 - precision_9: 0.7769 - recall_9: 0.5357\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x478b53210>"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_cnn.fit(word2vec_vectorized_train_corpus, trainset_target, epochs=30, batch_size=128, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNN + Pre-trained Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11314, 300)\n"
     ]
    }
   ],
   "source": [
    "pretrained_word2vec_cnn = create_cnn_model(input_shape=(maxlen, ))\n",
    "\n",
    "pretrained_word2vec_vectorized_train_corpus = pretrained_word2vec_vectorized_train_corpus.reshape(\n",
    "    pretrained_word2vec_vectorized_train_corpus.shape[0],  # number of samples\n",
    "    maxlen,  # word2vec embedding dimension (e.g., 300)\n",
    ")\n",
    "\n",
    "print(pretrained_word2vec_vectorized_train_corpus.shape)  # Should be (num_samples, sequence_length, embedding_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "89/89 - 14s - 156ms/step - accuracy: 0.2293 - loss: 2.4873 - precision_10: 0.5676 - recall_10: 0.0390\n",
      "Epoch 2/30\n",
      "89/89 - 13s - 150ms/step - accuracy: 0.4147 - loss: 1.7158 - precision_10: 0.6850 - recall_10: 0.2022\n",
      "Epoch 3/30\n",
      "89/89 - 13s - 151ms/step - accuracy: 0.4915 - loss: 1.5000 - precision_10: 0.7225 - recall_10: 0.2929\n",
      "Epoch 4/30\n",
      "89/89 - 14s - 153ms/step - accuracy: 0.5289 - loss: 1.3790 - precision_10: 0.7381 - recall_10: 0.3552\n",
      "Epoch 5/30\n",
      "89/89 - 14s - 154ms/step - accuracy: 0.5589 - loss: 1.3021 - precision_10: 0.7580 - recall_10: 0.3923\n",
      "Epoch 6/30\n",
      "89/89 - 14s - 153ms/step - accuracy: 0.5802 - loss: 1.2365 - precision_10: 0.7668 - recall_10: 0.4229\n",
      "Epoch 7/30\n",
      "89/89 - 14s - 153ms/step - accuracy: 0.6036 - loss: 1.1815 - precision_10: 0.7680 - recall_10: 0.4520\n",
      "Epoch 8/30\n",
      "89/89 - 14s - 152ms/step - accuracy: 0.6194 - loss: 1.1506 - precision_10: 0.7851 - recall_10: 0.4739\n",
      "Epoch 9/30\n",
      "89/89 - 13s - 151ms/step - accuracy: 0.6307 - loss: 1.0959 - precision_10: 0.7857 - recall_10: 0.4975\n",
      "Epoch 10/30\n",
      "89/89 - 13s - 150ms/step - accuracy: 0.6459 - loss: 1.0505 - precision_10: 0.7953 - recall_10: 0.5192\n",
      "Epoch 11/30\n",
      "89/89 - 14s - 156ms/step - accuracy: 0.6530 - loss: 1.0147 - precision_10: 0.7974 - recall_10: 0.5301\n",
      "Epoch 12/30\n",
      "89/89 - 14s - 156ms/step - accuracy: 0.6665 - loss: 0.9814 - precision_10: 0.8038 - recall_10: 0.5513\n",
      "Epoch 13/30\n",
      "89/89 - 14s - 163ms/step - accuracy: 0.6823 - loss: 0.9507 - precision_10: 0.8102 - recall_10: 0.5648\n",
      "Epoch 14/30\n",
      "89/89 - 15s - 174ms/step - accuracy: 0.6862 - loss: 0.9241 - precision_10: 0.8141 - recall_10: 0.5796\n",
      "Epoch 15/30\n",
      "89/89 - 15s - 164ms/step - accuracy: 0.7002 - loss: 0.8859 - precision_10: 0.8127 - recall_10: 0.5947\n",
      "Epoch 16/30\n",
      "89/89 - 15s - 163ms/step - accuracy: 0.7089 - loss: 0.8635 - precision_10: 0.8245 - recall_10: 0.6059\n",
      "Epoch 17/30\n",
      "89/89 - 15s - 163ms/step - accuracy: 0.7124 - loss: 0.8460 - precision_10: 0.8212 - recall_10: 0.6151\n",
      "Epoch 18/30\n",
      "89/89 - 19s - 219ms/step - accuracy: 0.7188 - loss: 0.8234 - precision_10: 0.8287 - recall_10: 0.6237\n",
      "Epoch 19/30\n",
      "89/89 - 14s - 161ms/step - accuracy: 0.7274 - loss: 0.7878 - precision_10: 0.8280 - recall_10: 0.6400\n",
      "Epoch 20/30\n",
      "89/89 - 14s - 157ms/step - accuracy: 0.7440 - loss: 0.7583 - precision_10: 0.8437 - recall_10: 0.6614\n",
      "Epoch 21/30\n",
      "89/89 - 14s - 152ms/step - accuracy: 0.7434 - loss: 0.7543 - precision_10: 0.8364 - recall_10: 0.6640\n",
      "Epoch 22/30\n",
      "89/89 - 13s - 151ms/step - accuracy: 0.7528 - loss: 0.7283 - precision_10: 0.8446 - recall_10: 0.6729\n",
      "Epoch 23/30\n",
      "89/89 - 13s - 151ms/step - accuracy: 0.7584 - loss: 0.7068 - precision_10: 0.8413 - recall_10: 0.6832\n",
      "Epoch 24/30\n",
      "89/89 - 14s - 154ms/step - accuracy: 0.7695 - loss: 0.6752 - precision_10: 0.8508 - recall_10: 0.6990\n",
      "Epoch 25/30\n",
      "89/89 - 13s - 145ms/step - accuracy: 0.7680 - loss: 0.6806 - precision_10: 0.8495 - recall_10: 0.6967\n",
      "Epoch 26/30\n",
      "89/89 - 13s - 144ms/step - accuracy: 0.7795 - loss: 0.6475 - precision_10: 0.8529 - recall_10: 0.7108\n",
      "Epoch 27/30\n",
      "89/89 - 14s - 155ms/step - accuracy: 0.7789 - loss: 0.6465 - precision_10: 0.8507 - recall_10: 0.7119\n",
      "Epoch 28/30\n",
      "89/89 - 14s - 153ms/step - accuracy: 0.7810 - loss: 0.6272 - precision_10: 0.8519 - recall_10: 0.7166\n",
      "Epoch 29/30\n",
      "89/89 - 13s - 152ms/step - accuracy: 0.7977 - loss: 0.5916 - precision_10: 0.8617 - recall_10: 0.7355\n",
      "Epoch 30/30\n",
      "89/89 - 13s - 152ms/step - accuracy: 0.7981 - loss: 0.5903 - precision_10: 0.8638 - recall_10: 0.7399\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x4344f2610>"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_word2vec_cnn.fit(pretrained_word2vec_vectorized_train_corpus, trainset_target, epochs=30, batch_size=128, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's evaluate our models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def evauate_model(model, X_test, Y_test, target_names):\n",
    "    \"\"\"Generates a report with metrics like precision, recall, f1, support for each class\"\"\"\n",
    "    y_pred = model.predict(X_test)\n",
    "    report = classification_report(Y_test, y_pred, target_names=target_names, output_dict=True)\n",
    "    report = pd.DataFrame(report).transpose()\n",
    "    return report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Evaluating Naive Bayes Classifier with TF-IDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>talk.politics.mideast</th>\n",
       "      <td>0.847368</td>\n",
       "      <td>0.504702</td>\n",
       "      <td>0.632613</td>\n",
       "      <td>319.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rec.autos</th>\n",
       "      <td>0.786517</td>\n",
       "      <td>0.719794</td>\n",
       "      <td>0.751678</td>\n",
       "      <td>389.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>comp.sys.mac.hardware</th>\n",
       "      <td>0.781609</td>\n",
       "      <td>0.690355</td>\n",
       "      <td>0.733154</td>\n",
       "      <td>394.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alt.atheism</th>\n",
       "      <td>0.689579</td>\n",
       "      <td>0.793367</td>\n",
       "      <td>0.737841</td>\n",
       "      <td>392.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rec.sport.baseball</th>\n",
       "      <td>0.859375</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.858257</td>\n",
       "      <td>385.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>comp.os.ms-windows.misc</th>\n",
       "      <td>0.893064</td>\n",
       "      <td>0.782278</td>\n",
       "      <td>0.834008</td>\n",
       "      <td>395.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rec.sport.hockey</th>\n",
       "      <td>0.945763</td>\n",
       "      <td>0.715385</td>\n",
       "      <td>0.814599</td>\n",
       "      <td>390.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sci.crypt</th>\n",
       "      <td>0.874704</td>\n",
       "      <td>0.934343</td>\n",
       "      <td>0.903541</td>\n",
       "      <td>396.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sci.med</th>\n",
       "      <td>0.926829</td>\n",
       "      <td>0.954774</td>\n",
       "      <td>0.940594</td>\n",
       "      <td>398.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>talk.politics.misc</th>\n",
       "      <td>0.946835</td>\n",
       "      <td>0.942065</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>397.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rec.motorcycles</th>\n",
       "      <td>0.942029</td>\n",
       "      <td>0.977444</td>\n",
       "      <td>0.959410</td>\n",
       "      <td>399.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>comp.windows.x</th>\n",
       "      <td>0.670753</td>\n",
       "      <td>0.967172</td>\n",
       "      <td>0.792141</td>\n",
       "      <td>396.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>comp.graphics</th>\n",
       "      <td>0.836013</td>\n",
       "      <td>0.661578</td>\n",
       "      <td>0.738636</td>\n",
       "      <td>393.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>comp.sys.ibm.pc.hardware</th>\n",
       "      <td>0.939130</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.874494</td>\n",
       "      <td>396.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sci.electronics</th>\n",
       "      <td>0.868735</td>\n",
       "      <td>0.923858</td>\n",
       "      <td>0.895449</td>\n",
       "      <td>394.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>talk.politics.guns</th>\n",
       "      <td>0.511229</td>\n",
       "      <td>0.972362</td>\n",
       "      <td>0.670130</td>\n",
       "      <td>398.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sci.space</th>\n",
       "      <td>0.619643</td>\n",
       "      <td>0.953297</td>\n",
       "      <td>0.751082</td>\n",
       "      <td>364.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>soc.religion.christian</th>\n",
       "      <td>0.916230</td>\n",
       "      <td>0.930851</td>\n",
       "      <td>0.923483</td>\n",
       "      <td>376.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>misc.forsale</th>\n",
       "      <td>0.959459</td>\n",
       "      <td>0.458065</td>\n",
       "      <td>0.620087</td>\n",
       "      <td>310.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>talk.religion.misc</th>\n",
       "      <td>0.962963</td>\n",
       "      <td>0.103586</td>\n",
       "      <td>0.187050</td>\n",
       "      <td>251.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.801779</td>\n",
       "      <td>0.801779</td>\n",
       "      <td>0.801779</td>\n",
       "      <td>0.801779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.838891</td>\n",
       "      <td>0.783030</td>\n",
       "      <td>0.778135</td>\n",
       "      <td>7532.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.835772</td>\n",
       "      <td>0.801779</td>\n",
       "      <td>0.792533</td>\n",
       "      <td>7532.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          precision    recall  f1-score      support\n",
       "talk.politics.mideast      0.847368  0.504702  0.632613   319.000000\n",
       "rec.autos                  0.786517  0.719794  0.751678   389.000000\n",
       "comp.sys.mac.hardware      0.781609  0.690355  0.733154   394.000000\n",
       "alt.atheism                0.689579  0.793367  0.737841   392.000000\n",
       "rec.sport.baseball         0.859375  0.857143  0.858257   385.000000\n",
       "comp.os.ms-windows.misc    0.893064  0.782278  0.834008   395.000000\n",
       "rec.sport.hockey           0.945763  0.715385  0.814599   390.000000\n",
       "sci.crypt                  0.874704  0.934343  0.903541   396.000000\n",
       "sci.med                    0.926829  0.954774  0.940594   398.000000\n",
       "talk.politics.misc         0.946835  0.942065  0.944444   397.000000\n",
       "rec.motorcycles            0.942029  0.977444  0.959410   399.000000\n",
       "comp.windows.x             0.670753  0.967172  0.792141   396.000000\n",
       "comp.graphics              0.836013  0.661578  0.738636   393.000000\n",
       "comp.sys.ibm.pc.hardware   0.939130  0.818182  0.874494   396.000000\n",
       "sci.electronics            0.868735  0.923858  0.895449   394.000000\n",
       "talk.politics.guns         0.511229  0.972362  0.670130   398.000000\n",
       "sci.space                  0.619643  0.953297  0.751082   364.000000\n",
       "soc.religion.christian     0.916230  0.930851  0.923483   376.000000\n",
       "misc.forsale               0.959459  0.458065  0.620087   310.000000\n",
       "talk.religion.misc         0.962963  0.103586  0.187050   251.000000\n",
       "accuracy                   0.801779  0.801779  0.801779     0.801779\n",
       "macro avg                  0.838891  0.783030  0.778135  7532.000000\n",
       "weighted avg               0.835772  0.801779  0.792533  7532.000000"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report = evauate_model(naive_bayes_classifier_tfidf, tfidf_vectorized_test_corpus, testset['target'], testset['target_names'])\n",
    "report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Evaluating Logistic Regression with TF-IDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>talk.politics.mideast</th>\n",
       "      <td>0.735915</td>\n",
       "      <td>0.655172</td>\n",
       "      <td>0.693201</td>\n",
       "      <td>319.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rec.autos</th>\n",
       "      <td>0.693182</td>\n",
       "      <td>0.784062</td>\n",
       "      <td>0.735826</td>\n",
       "      <td>389.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>comp.sys.mac.hardware</th>\n",
       "      <td>0.753247</td>\n",
       "      <td>0.736041</td>\n",
       "      <td>0.744544</td>\n",
       "      <td>394.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alt.atheism</th>\n",
       "      <td>0.717391</td>\n",
       "      <td>0.757653</td>\n",
       "      <td>0.736973</td>\n",
       "      <td>392.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rec.sport.baseball</th>\n",
       "      <td>0.860590</td>\n",
       "      <td>0.833766</td>\n",
       "      <td>0.846966</td>\n",
       "      <td>385.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>comp.os.ms-windows.misc</th>\n",
       "      <td>0.850144</td>\n",
       "      <td>0.746835</td>\n",
       "      <td>0.795148</td>\n",
       "      <td>395.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rec.sport.hockey</th>\n",
       "      <td>0.768349</td>\n",
       "      <td>0.858974</td>\n",
       "      <td>0.811138</td>\n",
       "      <td>390.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sci.crypt</th>\n",
       "      <td>0.911227</td>\n",
       "      <td>0.881313</td>\n",
       "      <td>0.896021</td>\n",
       "      <td>396.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sci.med</th>\n",
       "      <td>0.955959</td>\n",
       "      <td>0.927136</td>\n",
       "      <td>0.941327</td>\n",
       "      <td>398.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>talk.politics.misc</th>\n",
       "      <td>0.924812</td>\n",
       "      <td>0.929471</td>\n",
       "      <td>0.927136</td>\n",
       "      <td>397.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rec.motorcycles</th>\n",
       "      <td>0.952618</td>\n",
       "      <td>0.957393</td>\n",
       "      <td>0.955000</td>\n",
       "      <td>399.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>comp.windows.x</th>\n",
       "      <td>0.949602</td>\n",
       "      <td>0.904040</td>\n",
       "      <td>0.926261</td>\n",
       "      <td>396.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>comp.graphics</th>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.811705</td>\n",
       "      <td>0.770531</td>\n",
       "      <td>393.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>comp.sys.ibm.pc.hardware</th>\n",
       "      <td>0.886076</td>\n",
       "      <td>0.883838</td>\n",
       "      <td>0.884956</td>\n",
       "      <td>396.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sci.electronics</th>\n",
       "      <td>0.921717</td>\n",
       "      <td>0.926396</td>\n",
       "      <td>0.924051</td>\n",
       "      <td>394.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>talk.politics.guns</th>\n",
       "      <td>0.740286</td>\n",
       "      <td>0.909548</td>\n",
       "      <td>0.816234</td>\n",
       "      <td>398.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sci.space</th>\n",
       "      <td>0.718681</td>\n",
       "      <td>0.898352</td>\n",
       "      <td>0.798535</td>\n",
       "      <td>364.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>soc.religion.christian</th>\n",
       "      <td>0.964392</td>\n",
       "      <td>0.864362</td>\n",
       "      <td>0.911641</td>\n",
       "      <td>376.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>misc.forsale</th>\n",
       "      <td>0.757202</td>\n",
       "      <td>0.593548</td>\n",
       "      <td>0.665461</td>\n",
       "      <td>310.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>talk.religion.misc</th>\n",
       "      <td>0.757962</td>\n",
       "      <td>0.474104</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>251.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.827138</td>\n",
       "      <td>0.827138</td>\n",
       "      <td>0.827138</td>\n",
       "      <td>0.827138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.827634</td>\n",
       "      <td>0.816685</td>\n",
       "      <td>0.818214</td>\n",
       "      <td>7532.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.831103</td>\n",
       "      <td>0.827138</td>\n",
       "      <td>0.825792</td>\n",
       "      <td>7532.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          precision    recall  f1-score      support\n",
       "talk.politics.mideast      0.735915  0.655172  0.693201   319.000000\n",
       "rec.autos                  0.693182  0.784062  0.735826   389.000000\n",
       "comp.sys.mac.hardware      0.753247  0.736041  0.744544   394.000000\n",
       "alt.atheism                0.717391  0.757653  0.736973   392.000000\n",
       "rec.sport.baseball         0.860590  0.833766  0.846966   385.000000\n",
       "comp.os.ms-windows.misc    0.850144  0.746835  0.795148   395.000000\n",
       "rec.sport.hockey           0.768349  0.858974  0.811138   390.000000\n",
       "sci.crypt                  0.911227  0.881313  0.896021   396.000000\n",
       "sci.med                    0.955959  0.927136  0.941327   398.000000\n",
       "talk.politics.misc         0.924812  0.929471  0.927136   397.000000\n",
       "rec.motorcycles            0.952618  0.957393  0.955000   399.000000\n",
       "comp.windows.x             0.949602  0.904040  0.926261   396.000000\n",
       "comp.graphics              0.733333  0.811705  0.770531   393.000000\n",
       "comp.sys.ibm.pc.hardware   0.886076  0.883838  0.884956   396.000000\n",
       "sci.electronics            0.921717  0.926396  0.924051   394.000000\n",
       "talk.politics.guns         0.740286  0.909548  0.816234   398.000000\n",
       "sci.space                  0.718681  0.898352  0.798535   364.000000\n",
       "soc.religion.christian     0.964392  0.864362  0.911641   376.000000\n",
       "misc.forsale               0.757202  0.593548  0.665461   310.000000\n",
       "talk.religion.misc         0.757962  0.474104  0.583333   251.000000\n",
       "accuracy                   0.827138  0.827138  0.827138     0.827138\n",
       "macro avg                  0.827634  0.816685  0.818214  7532.000000\n",
       "weighted avg               0.831103  0.827138  0.825792  7532.000000"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report = evauate_model(logistic_regression_clf_tfidf, tfidf_vectorized_test_corpus, testset['target'], testset['target_names'])\n",
    "report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Evaluating Logistic Regression with Word2Vec Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>talk.politics.mideast</th>\n",
       "      <td>0.411594</td>\n",
       "      <td>0.445141</td>\n",
       "      <td>0.427711</td>\n",
       "      <td>319.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rec.autos</th>\n",
       "      <td>0.507576</td>\n",
       "      <td>0.516710</td>\n",
       "      <td>0.512102</td>\n",
       "      <td>389.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>comp.sys.mac.hardware</th>\n",
       "      <td>0.523923</td>\n",
       "      <td>0.555838</td>\n",
       "      <td>0.539409</td>\n",
       "      <td>394.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alt.atheism</th>\n",
       "      <td>0.507418</td>\n",
       "      <td>0.436224</td>\n",
       "      <td>0.469136</td>\n",
       "      <td>392.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rec.sport.baseball</th>\n",
       "      <td>0.475177</td>\n",
       "      <td>0.348052</td>\n",
       "      <td>0.401799</td>\n",
       "      <td>385.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>comp.os.ms-windows.misc</th>\n",
       "      <td>0.597964</td>\n",
       "      <td>0.594937</td>\n",
       "      <td>0.596447</td>\n",
       "      <td>395.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rec.sport.hockey</th>\n",
       "      <td>0.664160</td>\n",
       "      <td>0.679487</td>\n",
       "      <td>0.671736</td>\n",
       "      <td>390.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sci.crypt</th>\n",
       "      <td>0.506234</td>\n",
       "      <td>0.512626</td>\n",
       "      <td>0.509410</td>\n",
       "      <td>396.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sci.med</th>\n",
       "      <td>0.450820</td>\n",
       "      <td>0.552764</td>\n",
       "      <td>0.496614</td>\n",
       "      <td>398.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>talk.politics.misc</th>\n",
       "      <td>0.565217</td>\n",
       "      <td>0.556675</td>\n",
       "      <td>0.560914</td>\n",
       "      <td>397.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rec.motorcycles</th>\n",
       "      <td>0.708010</td>\n",
       "      <td>0.686717</td>\n",
       "      <td>0.697201</td>\n",
       "      <td>399.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>comp.windows.x</th>\n",
       "      <td>0.735695</td>\n",
       "      <td>0.681818</td>\n",
       "      <td>0.707733</td>\n",
       "      <td>396.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>comp.graphics</th>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.427481</td>\n",
       "      <td>0.435798</td>\n",
       "      <td>393.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>comp.sys.ibm.pc.hardware</th>\n",
       "      <td>0.519814</td>\n",
       "      <td>0.563131</td>\n",
       "      <td>0.540606</td>\n",
       "      <td>396.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sci.electronics</th>\n",
       "      <td>0.544811</td>\n",
       "      <td>0.586294</td>\n",
       "      <td>0.564792</td>\n",
       "      <td>394.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>talk.politics.guns</th>\n",
       "      <td>0.550102</td>\n",
       "      <td>0.675879</td>\n",
       "      <td>0.606539</td>\n",
       "      <td>398.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sci.space</th>\n",
       "      <td>0.443678</td>\n",
       "      <td>0.530220</td>\n",
       "      <td>0.483104</td>\n",
       "      <td>364.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>soc.religion.christian</th>\n",
       "      <td>0.639798</td>\n",
       "      <td>0.675532</td>\n",
       "      <td>0.657180</td>\n",
       "      <td>376.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>misc.forsale</th>\n",
       "      <td>0.434783</td>\n",
       "      <td>0.322581</td>\n",
       "      <td>0.370370</td>\n",
       "      <td>310.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>talk.religion.misc</th>\n",
       "      <td>0.369863</td>\n",
       "      <td>0.215139</td>\n",
       "      <td>0.272040</td>\n",
       "      <td>251.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.537307</td>\n",
       "      <td>0.537307</td>\n",
       "      <td>0.537307</td>\n",
       "      <td>0.537307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.530054</td>\n",
       "      <td>0.528162</td>\n",
       "      <td>0.526032</td>\n",
       "      <td>7532.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.535601</td>\n",
       "      <td>0.537307</td>\n",
       "      <td>0.533746</td>\n",
       "      <td>7532.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          precision    recall  f1-score      support\n",
       "talk.politics.mideast      0.411594  0.445141  0.427711   319.000000\n",
       "rec.autos                  0.507576  0.516710  0.512102   389.000000\n",
       "comp.sys.mac.hardware      0.523923  0.555838  0.539409   394.000000\n",
       "alt.atheism                0.507418  0.436224  0.469136   392.000000\n",
       "rec.sport.baseball         0.475177  0.348052  0.401799   385.000000\n",
       "comp.os.ms-windows.misc    0.597964  0.594937  0.596447   395.000000\n",
       "rec.sport.hockey           0.664160  0.679487  0.671736   390.000000\n",
       "sci.crypt                  0.506234  0.512626  0.509410   396.000000\n",
       "sci.med                    0.450820  0.552764  0.496614   398.000000\n",
       "talk.politics.misc         0.565217  0.556675  0.560914   397.000000\n",
       "rec.motorcycles            0.708010  0.686717  0.697201   399.000000\n",
       "comp.windows.x             0.735695  0.681818  0.707733   396.000000\n",
       "comp.graphics              0.444444  0.427481  0.435798   393.000000\n",
       "comp.sys.ibm.pc.hardware   0.519814  0.563131  0.540606   396.000000\n",
       "sci.electronics            0.544811  0.586294  0.564792   394.000000\n",
       "talk.politics.guns         0.550102  0.675879  0.606539   398.000000\n",
       "sci.space                  0.443678  0.530220  0.483104   364.000000\n",
       "soc.religion.christian     0.639798  0.675532  0.657180   376.000000\n",
       "misc.forsale               0.434783  0.322581  0.370370   310.000000\n",
       "talk.religion.misc         0.369863  0.215139  0.272040   251.000000\n",
       "accuracy                   0.537307  0.537307  0.537307     0.537307\n",
       "macro avg                  0.530054  0.528162  0.526032  7532.000000\n",
       "weighted avg               0.535601  0.537307  0.533746  7532.000000"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report = evauate_model(logistic_regression_clf_word2vec, word2vec_vectorized_test_corpus, testset['target'], testset['target_names'])\n",
    "report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Evaluating Logistic Regression with Pre-trained Word2Vec Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>talk.politics.mideast</th>\n",
       "      <td>0.489676</td>\n",
       "      <td>0.520376</td>\n",
       "      <td>0.504559</td>\n",
       "      <td>319.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rec.autos</th>\n",
       "      <td>0.589327</td>\n",
       "      <td>0.652956</td>\n",
       "      <td>0.619512</td>\n",
       "      <td>389.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>comp.sys.mac.hardware</th>\n",
       "      <td>0.561743</td>\n",
       "      <td>0.588832</td>\n",
       "      <td>0.574969</td>\n",
       "      <td>394.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alt.atheism</th>\n",
       "      <td>0.568063</td>\n",
       "      <td>0.553571</td>\n",
       "      <td>0.560724</td>\n",
       "      <td>392.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rec.sport.baseball</th>\n",
       "      <td>0.596491</td>\n",
       "      <td>0.529870</td>\n",
       "      <td>0.561210</td>\n",
       "      <td>385.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>comp.os.ms-windows.misc</th>\n",
       "      <td>0.653846</td>\n",
       "      <td>0.645570</td>\n",
       "      <td>0.649682</td>\n",
       "      <td>395.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rec.sport.hockey</th>\n",
       "      <td>0.738903</td>\n",
       "      <td>0.725641</td>\n",
       "      <td>0.732212</td>\n",
       "      <td>390.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sci.crypt</th>\n",
       "      <td>0.792208</td>\n",
       "      <td>0.770202</td>\n",
       "      <td>0.781050</td>\n",
       "      <td>396.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sci.med</th>\n",
       "      <td>0.781022</td>\n",
       "      <td>0.806533</td>\n",
       "      <td>0.793572</td>\n",
       "      <td>398.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>talk.politics.misc</th>\n",
       "      <td>0.855037</td>\n",
       "      <td>0.876574</td>\n",
       "      <td>0.865672</td>\n",
       "      <td>397.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rec.motorcycles</th>\n",
       "      <td>0.929648</td>\n",
       "      <td>0.927318</td>\n",
       "      <td>0.928482</td>\n",
       "      <td>399.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>comp.windows.x</th>\n",
       "      <td>0.791667</td>\n",
       "      <td>0.767677</td>\n",
       "      <td>0.779487</td>\n",
       "      <td>396.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>comp.graphics</th>\n",
       "      <td>0.592875</td>\n",
       "      <td>0.592875</td>\n",
       "      <td>0.592875</td>\n",
       "      <td>393.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>comp.sys.ibm.pc.hardware</th>\n",
       "      <td>0.862694</td>\n",
       "      <td>0.840909</td>\n",
       "      <td>0.851662</td>\n",
       "      <td>396.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sci.electronics</th>\n",
       "      <td>0.825000</td>\n",
       "      <td>0.837563</td>\n",
       "      <td>0.831234</td>\n",
       "      <td>394.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>talk.politics.guns</th>\n",
       "      <td>0.651751</td>\n",
       "      <td>0.841709</td>\n",
       "      <td>0.734649</td>\n",
       "      <td>398.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sci.space</th>\n",
       "      <td>0.584582</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.657040</td>\n",
       "      <td>364.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>soc.religion.christian</th>\n",
       "      <td>0.846995</td>\n",
       "      <td>0.824468</td>\n",
       "      <td>0.835580</td>\n",
       "      <td>376.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>misc.forsale</th>\n",
       "      <td>0.567669</td>\n",
       "      <td>0.487097</td>\n",
       "      <td>0.524306</td>\n",
       "      <td>310.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>talk.religion.misc</th>\n",
       "      <td>0.506667</td>\n",
       "      <td>0.151394</td>\n",
       "      <td>0.233129</td>\n",
       "      <td>251.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.698619</td>\n",
       "      <td>0.698619</td>\n",
       "      <td>0.698619</td>\n",
       "      <td>0.698619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.689293</td>\n",
       "      <td>0.684557</td>\n",
       "      <td>0.680580</td>\n",
       "      <td>7532.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.696691</td>\n",
       "      <td>0.698619</td>\n",
       "      <td>0.692915</td>\n",
       "      <td>7532.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          precision    recall  f1-score      support\n",
       "talk.politics.mideast      0.489676  0.520376  0.504559   319.000000\n",
       "rec.autos                  0.589327  0.652956  0.619512   389.000000\n",
       "comp.sys.mac.hardware      0.561743  0.588832  0.574969   394.000000\n",
       "alt.atheism                0.568063  0.553571  0.560724   392.000000\n",
       "rec.sport.baseball         0.596491  0.529870  0.561210   385.000000\n",
       "comp.os.ms-windows.misc    0.653846  0.645570  0.649682   395.000000\n",
       "rec.sport.hockey           0.738903  0.725641  0.732212   390.000000\n",
       "sci.crypt                  0.792208  0.770202  0.781050   396.000000\n",
       "sci.med                    0.781022  0.806533  0.793572   398.000000\n",
       "talk.politics.misc         0.855037  0.876574  0.865672   397.000000\n",
       "rec.motorcycles            0.929648  0.927318  0.928482   399.000000\n",
       "comp.windows.x             0.791667  0.767677  0.779487   396.000000\n",
       "comp.graphics              0.592875  0.592875  0.592875   393.000000\n",
       "comp.sys.ibm.pc.hardware   0.862694  0.840909  0.851662   396.000000\n",
       "sci.electronics            0.825000  0.837563  0.831234   394.000000\n",
       "talk.politics.guns         0.651751  0.841709  0.734649   398.000000\n",
       "sci.space                  0.584582  0.750000  0.657040   364.000000\n",
       "soc.religion.christian     0.846995  0.824468  0.835580   376.000000\n",
       "misc.forsale               0.567669  0.487097  0.524306   310.000000\n",
       "talk.religion.misc         0.506667  0.151394  0.233129   251.000000\n",
       "accuracy                   0.698619  0.698619  0.698619     0.698619\n",
       "macro avg                  0.689293  0.684557  0.680580  7532.000000\n",
       "weighted avg               0.696691  0.698619  0.692915  7532.000000"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report = evauate_model(logistic_regression_clf_pretrained_word2vec, pretrained_word2vec_vectorized_test_corpus, testset['target'], testset['target_names'])\n",
    "report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Evaluating CNN with TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m236/236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 65ms/step - accuracy: 0.0593 - loss: 2.9857 - precision_8: 0.0000e+00 - recall_8: 0.0000e+00\n",
      "Test Loss: 2.9903924465179443\n",
      "Test Accuracy: 0.05297397822141647\n",
      "Test Precision: 0.0\n",
      "Test Recall: 0.0\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy, test_precision, test_recall = tfidf_cnn.evaluate(tfidf_vectorized_test_corpus_padded, testset_target, verbose=1)\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test Accuracy: {test_accuracy}\")\n",
    "print(f\"Test Precision: {test_precision}\")\n",
    "print(f\"Test Recall: {test_recall}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Evaluating CNN with Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m236/236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.5328 - loss: 1.6312 - precision_9: 0.6419 - recall_9: 0.4516\n",
      "Test Loss: 1.80009925365448\n",
      "Test Accuracy: 0.4779607057571411\n",
      "Test Precision: 0.5769230723381042\n",
      "Test Recall: 0.4002920985221863\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy, test_precision, test_recall = word2vec_cnn.evaluate(word2vec_vectorized_test_corpus, testset_target, verbose=1)\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test Accuracy: {test_accuracy}\")\n",
    "print(f\"Test Precision: {test_precision}\")\n",
    "print(f\"Test Recall: {test_recall}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Evaluating CNN with Pretrained Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m236/236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.5639 - loss: 1.8119 - precision_10: 0.6400 - recall_10: 0.5250\n",
      "Test Loss: 1.6807277202606201\n",
      "Test Accuracy: 0.5841742157936096\n",
      "Test Precision: 0.6551558971405029\n",
      "Test Recall: 0.5440785884857178\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy, test_precision, test_recall = pretrained_word2vec_cnn.evaluate(pretrained_word2vec_vectorized_test_corpus, testset_target, verbose=1)\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test Accuracy: {test_accuracy}\")\n",
    "print(f\"Test Precision: {test_precision}\")\n",
    "print(f\"Test Recall: {test_recall}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
